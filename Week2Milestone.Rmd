---
title: "Week 2 Milestone"
author: "Sarah E. Wright"
date: "Dec 29, 2017"
output: html_document
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(quanteda)
library(data.table)
library(plotly)
library(readtext)
library(RColorBrewer)
source("DataImport.R")
source("MakeNGrams.R")
working.dir <- "C:/Users/sewright/Documents/R/Classes/CourseraDataScienceCapstone"
setwd(working.dir)
```

## Data Import and Processing

Because the original data set is so large (see below), I only imported a 5% sample for the purpose of exploratory data analysis. Using the `quanteda` package, I created a corpus from my sample and generated some basic summary plots and statistics.
```{r dataprocessing, echo = FALSE, message = FALSE}
line.counts <- DataImport(working.dir)
MakeNGrams(working.dir)
# Line count for each file
line.counts
```

## Exploratory Data Analysis
To get a sense of which tokens appear most frequently in the corpus, I created a document-feature matrix using `quanteda`. I excluded "stop words" (common function words such as "a," "the," "I," "we," etc.) since I was more interested in getting a sense of which topics and themes were most common.
The fifty most common words in the corpus and their frequencies are as follows:
```{r summary, message = FALSE}

# Import data
path.to.data <- paste0(getwd(), "/data/final/en_US/")
text.files <- paste0(path.to.data, dir(path = path.to.data, pattern = "sample"))

# Create a corpus using the quanteda package
corpus.all <- corpus(readtext(text.files))
corpus.summary <- summary(corpus.all)

# List top 50 words in the corpus
my.dfm <- dfm(corpus.all, remove = stopwords("english"), remove_punct = TRUE, remove_numbers = TRUE)
topfeatures(my.dfm, 50)/sum(corpus.summary$Tokens)
```
Below is a word cloud of words that appear more than 1000 times in the corpus:
```{r}
textplot_wordcloud(my.dfm, scale = c(2.5, .2), min.freq = 1000, colors = brewer.pal(8, "Dark2"), random.order = FALSE)
```

```{r include = FALSE, message = FALSE}
# Plot showing size of object vs % of words covered

# Read in 1- through 4-grams
one.grams <- fread("one_grams.txt")
two.grams <- fread("two_grams.txt")
three.grams <- fread("three_grams.txt")
four.grams <- fread("four_grams.txt")

# Calculate total occurrences of all n-grams
n.one.grams <- sum(one.grams$count)
n.two.grams <- sum(two.grams$count)
n.three.grams <- sum(three.grams$count)
n.four.grams <- sum(four.grams$count)

# Print number of n-grams that cover 90% of occurrences
nrow(one.grams[cumulative.proportion <= 0.90])
nrow(two.grams[cumulative.proportion <= 0.90])
nrow(three.grams[cumulative.proportion <= 0.90])
nrow(four.grams[cumulative.proportion <= 0.90])

# Plot file size vs percentage of corpus covered
acc.data <- data.frame()
for (pct in seq(0.1, 0.9, 0.1)) {
    one.grams.size <- object.size(one.grams[cumulative.proportion <= pct])
    two.grams.size <- object.size(two.grams[cumulative.proportion <= pct])
    three.grams.size <- object.size(three.grams[cumulative.proportion <= pct])
    four.grams.size <- object.size(four.grams[cumulative.proportion <= pct])
    acc.data <- rbind(acc.data, c(pct, one.grams.size, two.grams.size, three.grams.size, four.grams.size))
}
names(acc.data) <- c("proportion", "one.grams.size", "two.grams.size", "three.grams.size", "four.grams.size")
acc.data[,2:5] <- acc.data[,2:5]/1000000000

plot_ly(data = acc.data, x = ~proportion) %>%
    add_trace(y = ~one.grams.size, name = "one-grams size (Gb)") %>%
    add_trace(y = ~two.grams.size, name = "two-grams size (Gb)") %>%
    add_trace(y = ~three.grams.size, name = "three-grams size (Gb)") %>%
    add_trace(y = ~four.grams.size, name = "four-grams size (Gb)")

# Plots of n-gram frequencies

```
 

## Prediction Algorithm and App

### Algorithm
I plan to use the interpolated Kneser-Ney algorithm. This algorithm calculates word probabilities based not only on how frequently a word completes n-grams, but also the number of unique n-grams the word completes. This ensures that a word that typically only occurs as part of a common phrase is not assigned a high probability in other contexts. Word probabilities are calculated using all n-gram orders; higher-order n-gram counts have more weight when they are large, whereas small, higher-order n-gram counts may be weighted less than large, lower-order n-gram counts. 

### Shiny App
The Shiny app will have two modes: autocompletion and text generation. The autocompletion mode will prompt the user to begin typing a sentence or phrase. When the user presses the "predict" button, the app will predict the next word in the sequence. The text generation mode will prompt the user for a short phrase and the number of sentences they wish to generate. Using the input phrase as a "seed," the app will predict the next word and then continue feeding the last three words of generated text back into the prediction algorithm until the requested number of sentences have been produced.